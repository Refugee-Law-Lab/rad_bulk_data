{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to parse text files to produce cleaned text of RAD decisions\n",
    "\n",
    "Sean Rehaag\n",
    "\n",
    "License: Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). \n",
    "\n",
    "Dataset & Code to be cited as:\n",
    "\n",
    "Sean Rehaag, \"Refugee Appeal Division Bulk Decisions Dataset\" (2023), online: Refugee Law Laboratory <https://refugeelab.ca/bulk-data/rad/>.\n",
    "\n",
    "Notes:\n",
    "\n",
    "(1) Data Source: Immigration and Refugee Board. In the Fall of 2022, the IRB added the Refugee Law Laboratory to their email distribution list for legal publishers of RAD decisions. The RLL therefore receives new RAD cases as they are released for publication by the IRB. Also, in the fall of 2022 the Immigration and Refugee Board provided the RLL with a full backlog of approximately 116k published decisions from all divisions (RAD, RPD, ID, IAD). \n",
    "\n",
    "(2) Unofficial Data: The data are unofficial reproductions. For official versions, please contact the Immigration and Refugee Board. \n",
    "\n",
    "(3) Non-Affiliation / Endorsement: The data has been collected and reproduced without any affiliation or endorsement from the Immigration and Refugee Board.\n",
    "\n",
    "(4) Non-Commerical Use: As indicated in the license, data may be used for non-commercial use (with attribution) only. For commercial use, please contact the Immigration and Refugee Board. \n",
    "\n",
    "(5) Accuracy: Data was collected and processed programmatically for the purposes of academic research. While we make best efforts to ensure accuracy, data gathering of this kind inevitably involves errors. As such the data should be viewed as preliminary information aimed to prompt further research and discussion, rather than as definitive information.\n",
    "\n",
    "Acknowledgements: Thanks to Rafael Dolores for coding the parsing scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install regex\n",
    "#!pip install dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langdetect import detect, DetectorFactory\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Constant\n",
    "Here, we specify the directories containing our data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIRS = [\"../RAD Decisions TEXT\", \"../IRB Decisions - Initial Request - TEXT\"]\n",
    "\n",
    "# For SR:\n",
    "DATA_DIRS = [\"d:/RAD Decisions TEXT/\", \"d:/IRB Decisions - Initial Request - TEXT/\"]\n",
    "\n",
    "# TEMMP for testing:\n",
    "\n",
    "#DATA_DIRS = [\"d:/RAD Decisions TEXT/\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for langdetect for consistent results and reproducibility\n",
    "DetectorFactory.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Language Detection\n",
    "This function determines the language of a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regular Expression Detector\n",
    "Functions to parse the date from text files while accounting for several different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_match_date_patterns(content):\n",
    "    patterns = {\n",
    "        \"custom\": (r\"Date (?:of decision|de la décision)\\s*\\n\\s*([A-Za-z]+)\\s+(\\d{1,2})\\.\\s*(\\d{4})\", lambda m: [m.group(1), m.group(2), m.group(3)]),\n",
    "        \"primary\": (r\"Date (?:of decision|de la décision)\\s*(?:Le )?\\s*((?:(?:\\d{1,2}|1er)\\s+[\\w]+\\s*,?\\s*\\d{1,4})|\\w+\\s+\\d{1,2}(?:st|nd|rd|th)?\\s*,?\\s*\\d{1,4}|\\d{1,2}-\\d{1,2}-\\d{1,4})\", lambda m: m.group(1).replace(',', '').split()),\n",
    "        \"original_decision\": (r\"Date of decision\\s+([\\w\\s]+),\\s+(\\d{4})\\s+\\(original decision\\)\", lambda m: m.group(1).strip().split() + [m.group(2).strip()]),\n",
    "        \"tribunal\": (r\"Tribunal\\s*\\n\\s*([\\w\\s]+?)\\s*\\n\\s*Date of decision\", lambda m: m.group(1).replace(',', '').split()),\n",
    "        \"original\": (r\"Original\\s+([\\w]+\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4})\", lambda m: m.group(1).replace(',', '').split())\n",
    "    }\n",
    "\n",
    "    for key, (pattern, process) in patterns.items():\n",
    "        match = re.search(pattern, content, re.IGNORECASE)\n",
    "        if match:\n",
    "            return process(match)\n",
    "    return None\n",
    "\n",
    "def match_date_patterns(content):\n",
    "    # 1. Try your proven patterns first\n",
    "    result = original_match_date_patterns(content)\n",
    "    if result:\n",
    "        return result\n",
    "\n",
    "    # 2. Fallback: Check lines after the label\n",
    "    lines = content.splitlines()\n",
    "    label_regex = re.compile(\n",
    "        r'^\\s*Date (of decision|de la décision)(\\s*and reasons|\\s*et des motifs)?\\s*$', re.IGNORECASE)\n",
    "\n",
    "    date_regexes = [\n",
    "        # French style: 1er mai 2015, 24 juillet 2018, Le 24 juillet 2018\n",
    "        re.compile(r'^(Le\\s+)?(\\d{1,2}|1er)\\s+\\w+\\s+\\d{4}$', re.IGNORECASE),\n",
    "        # English style: January 21, 2019 or November 5th, 2014\n",
    "        re.compile(r'^[A-Za-z]+\\s+\\d{1,2}(st|nd|rd|th)?\\,?\\s+\\d{4}$'),\n",
    "        # Dashes (rare but sometimes): 10-12-2018\n",
    "        re.compile(r'^\\d{1,2}-\\d{1,2}-\\d{4}$'),\n",
    "    ]\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if label_regex.match(line):\n",
    "            # Look ahead at the next 3 non-empty lines\n",
    "            lookahead = 0\n",
    "            for j in range(idx+1, min(idx+5, len(lines))):\n",
    "                candidate = lines[j].strip()\n",
    "                if not candidate:\n",
    "                    continue\n",
    "                lookahead += 1\n",
    "                for reg in date_regexes:\n",
    "                    m = reg.match(candidate)\n",
    "                    if m:\n",
    "                        cleaned = candidate.replace(\"Le \", \"\").replace(\",\", \"\")\n",
    "                        return cleaned.split()\n",
    "                if lookahead >= 3:\n",
    "                    break\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Formatter\n",
    "Takes detected regular expression and converts into one common format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_to_english = {\n",
    "        'janvier': 'January', 'fevrier': 'February', 'mars': 'March', 'avril': 'April',\n",
    "        'mai': 'May', 'juin': 'June', 'juillet': 'July', 'aout': 'August',\n",
    "        'septembre': 'September', 'octobre': 'October', 'novembre': 'November', 'decembre': 'December'\n",
    "}\n",
    "\n",
    "def correct_month_name(misspelled_month, possibilities=['Janvier','January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], cutoff=0.6):\n",
    "    correct_months = get_close_matches(misspelled_month, possibilities, n=1, cutoff=cutoff)\n",
    "    if correct_months:\n",
    "        corrected_month = correct_months[0]\n",
    "        # Check if the corrected month is in the French to English mapping\n",
    "        return french_to_english.get(corrected_month.lower(), corrected_month)\n",
    "    else:\n",
    "        return misspelled_month\n",
    "\n",
    "def correct_year_typo(year):\n",
    "    if len(year) == 3 and year.startswith(\"0\"):\n",
    "        return \"20\" + year[1:]\n",
    "    return year\n",
    "\n",
    "def correct_year_typo(year):\n",
    "    \"\"\"Corrects year format typos.\"\"\"\n",
    "    return \"20\" + year[1:] if len(year) == 3 and year.startswith(\"0\") else year\n",
    "\n",
    "def process_numeric_format(parts):\n",
    "    \"\"\"Processes numeric date format 'dd-mm-yyyy'.\"\"\"\n",
    "    day, month, year = parts[0].split('-')\n",
    "    year = correct_year_typo(year)\n",
    "    return datetime(int(year), int(month), int(day)).date().strftime('%Y-%m-%d')\n",
    "\n",
    "def process_day_first_format(parts, french_month_mapping):\n",
    "    \"\"\"Processes dates in 'day month year' format, French or English.\"\"\"\n",
    "    day = 1 if parts[0].lower() == '1er' else int(parts[0])\n",
    "\n",
    "    month = ''\n",
    "    # Check if month and year are concatenated\n",
    "    if len(parts) == 2 and not parts[1].isdigit():\n",
    "        month_year_str = parts[1]\n",
    "        for i in range(1, len(month_year_str)):\n",
    "            if month_year_str[i:].isdigit():\n",
    "                month_str, year_str = month_year_str[:i], month_year_str[i:]\n",
    "                year = correct_year_typo(year_str)\n",
    "                month = french_month_mapping.get(month_str.lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o'), month_str.capitalize())\n",
    "                break\n",
    "    else:\n",
    "        month = parts[1].lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o')\n",
    "        year = correct_year_typo(parts[2])\n",
    "\n",
    "    if month in french_month_mapping:\n",
    "        return datetime(int(year), french_month_mapping[month], day).date().strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        if isinstance(month, int):\n",
    "            return datetime(int(year), month, day).date().strftime('%Y-%m-%d')\n",
    "        \n",
    "        corrected_month = correct_month_name(month.capitalize())\n",
    "        try:\n",
    "            return datetime.strptime(f\"{corrected_month} {day} {year}\", '%B %d %Y').date().strftime('%Y-%m-%d')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date: {e}\")\n",
    "            return None\n",
    "\n",
    "def process_month_first_format(parts):\n",
    "    \"\"\"Processes month first format with possible ordinal suffix.\"\"\"\n",
    "    day = 0\n",
    "    month = ''\n",
    "    year = ''\n",
    "    \n",
    "    if len(parts) == 2 and parts[1].isdigit() and len(parts[1]) > 2:\n",
    "        \n",
    "        if parts[1].isdigit() and len(parts[1]) > 4: \n",
    "            month = parts[0]\n",
    "            year_str = parts[1][-4:]\n",
    "            day_str = parts[1][:-4]\n",
    "            year = year_str\n",
    "            day = int(day_str)\n",
    "            \n",
    "        elif parts[1].isdigit()and len(parts[1]) > 3: #Year is the second entry\n",
    "            month_day_str = parts[0]\n",
    "            for i in range(1, len(month_day_str)):\n",
    "                if not month_day_str[i].isdigit():\n",
    "                    day_str, month_str = month_day_str[:i], month_day_str[i:]\n",
    "                    day = int(day_str)\n",
    "                    month = french_to_english.get(month_str.lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o'), month_str)\n",
    "                    parts[0] = month\n",
    "                    break\n",
    "            year = parts[1]\n",
    "        else:\n",
    "            year_str = parts[1][-4:]\n",
    "            day_str = parts[1][:-4]\n",
    "            year = correct_year_typo(year_str)\n",
    "            day = int(day_str)\n",
    "\n",
    "    else:\n",
    "        day = re.sub(r\"[^\\d]\", \"\", parts[1])\n",
    "        day = int(day) if day.isdigit() else 1\n",
    "        year = correct_year_typo(parts[2])\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        corrected_month = correct_month_name(parts[0].capitalize())\n",
    "        return datetime.strptime(f\"{corrected_month} {day} {year}\", '%B %d %Y').date().strftime('%Y-%m-%d')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing date: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Date Extraction\n",
    "This function searches the given file for the document date using regular expressions, taking into account both French and English texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_parts(parts, french_month_mapping):\n",
    "    \"\"\"Determines the correct date processing method based on the format of the parts.\"\"\"\n",
    "    if '-' in parts[0]:\n",
    "        return process_numeric_format(parts)\n",
    "    elif parts[0].isdigit() or parts[0].lower() == '1er':\n",
    "        return process_day_first_format(parts, french_month_mapping)\n",
    "    else:\n",
    "        return process_month_first_format(parts)\n",
    "\n",
    "def extract_document_date(content):\n",
    "    french_month_mapping = {\n",
    "        'janvier': 1, 'fevrier': 2, 'mars': 3, 'avril': 4,\n",
    "        'mai': 5, 'juin': 6, 'juillet': 7, 'aout': 8,\n",
    "        'septembre': 9, 'octobre': 10, 'novembre': 11, 'decembre': 12\n",
    "    }\n",
    "    \n",
    "    parts = match_date_patterns(content)\n",
    "    \n",
    "    if not parts:\n",
    "        return None\n",
    "    return process_date_parts(parts, french_month_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processor Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rad_number(content):\n",
    "    \"\"\"Extracts the RAD number from the content, ignoring IAD files.\"\"\"\n",
    "     \n",
    "    # Check for lines indicating the file should be ignored\n",
    "    ignore_lines = [\"IAD File\",\n",
    "                    \"IMMIGRATION APPEAL DIVISION\", \n",
    "                    \"ID File\", \n",
    "                    \"IMMIGRATION DIVISION\", \n",
    "                    \"RPD File\", \n",
    "                    \"RPD file\",\n",
    "                    \"REFUGEE PROTECTION DIVISION\", \n",
    "                    \"REFUGEE DIVISION\"\n",
    "                    ]\n",
    "    \n",
    "    \n",
    "    for line in content.splitlines():\n",
    "     \n",
    "        if any(ignore_line in line for ignore_line in ignore_lines):       \n",
    "            return None\n",
    "                \n",
    "        sanitized_line = ''.join(c for c in line if c.isprintable()).strip()\n",
    "    \n",
    "        if \"RAD File\" in sanitized_line :\n",
    "            \n",
    "            rad_number_match = re.search(r\"([A-Z]{2}\\d+.\\d+)\", sanitized_line)\n",
    "            if rad_number_match:\n",
    "                return rad_number_match.group(1)\n",
    "            \n",
    "            # If RAD is in the next immediate line\n",
    "            next_line_index = content.splitlines().index(line) + 1\n",
    "            if next_line_index < len(content.splitlines()):\n",
    "                next_line = content.splitlines()[next_line_index]\n",
    "                rad_number_match = re.search(r\"([A-Z]{2}\\d+-\\d+)\", next_line)\n",
    "                if rad_number_match:\n",
    "                    return rad_number_match.group(1)\n",
    "        \n",
    "            # fallback to catch a common error\n",
    "\n",
    "        if \"SAR File\" in sanitized_line :\n",
    "            rad_number_match = re.search(r\"([A-Z]{2}\\d+.\\d+)\", sanitized_line)\n",
    "            if rad_number_match:\n",
    "                return rad_number_match.group(1)\n",
    "            \n",
    "            # If RAD is in the next immediate line\n",
    "            next_line_index = content.splitlines().index(line) + 1\n",
    "            if next_line_index < len(content.splitlines()):\n",
    "                next_line = content.splitlines()[next_line_index]\n",
    "                rad_number_match = re.search(r\"([A-Z]{2}\\d+-\\d+)\", next_line)\n",
    "                if rad_number_match:\n",
    "                    return rad_number_match.group(1)\n",
    "              \n",
    "    return None\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "\n",
    "    try:\n",
    "        \"\"\"Processes a single file and extracts data.\"\"\"\n",
    "\n",
    "        # Use chardet to detect the encoding of the file\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "        # Read the file with the detected encoding\n",
    "        with open(file_path, 'r', errors='replace', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Map problematic file fragments to their corresponding RAD numbers\n",
    "\n",
    "        problem_files = {\n",
    "            \"MC0-01686ta.txt\": \"MC0-01686\",\n",
    "            \"MC0-04831ta.txt\": \"MC0-04831\",\n",
    "            \"MC0-06114tf.txt\": \"MC0-06114\",\n",
    "            \"MC0-08063ta.txt\": \"MC0-08063\",\n",
    "            \"MC0-08521 f.txt\": \"MC0-08521\",\n",
    "            \"MC0-10174ta.txt\": \"MC0-10174\",\n",
    "            \"MC0-10320tf.txt\": \"MC0-10320\",\n",
    "            \"MC1-00352 tf.txt\": \"MC1-00352\",\n",
    "            \"TB8-11442tf.txt\": \"TB8-11442\",\n",
    "            \"TB9-10382tf.txt\": \"TB9-10382\",\n",
    "            \"TB9-29568tf.txt\": \"TB9-29568\",\n",
    "            \"TC0-05851tf.txt\": \"TC0-05851\",\n",
    "            \"TC0-10233 a.txt\": \"TC0-10233\",\n",
    "            \"TC1-04783tf.txt\": \"TC1-04783\",\n",
    "            \"TC1-12335 a.txt\": \"TC1-12335\",\n",
    "            \"TC1-12335 tf.txt\": \"TC1-12335\",\n",
    "            \"1691090.txt\": \"MB4-00757\",\n",
    "            \"1747179.txt\": \"MB4-02189\",\n",
    "            \"1747181.txt\": \"MB4-03314\",\n",
    "            \"1797799.txt\": \"MB3-03350\",\n",
    "            \"1802396.txt\": \"VB4-01077\",\n",
    "            \"1802506.txt\": \"VB3-02589\",\n",
    "            \"1820441.txt\": \"MB4-04618\",\n",
    "            \"1825243.txt\": \"MB4-02540\",\n",
    "            \"1825447.txt\": \"VB4-01870\",\n",
    "            \"2294273.txt\": \"MB6-05587\",\n",
    "            \"2294277.txt\": \"TB5-02604\",\n",
    "            \"2685118.txt\": \"VB5-00414\",\n",
    "            \"2714254.txt\": \"TB5-10940\",\n",
    "            \"2726259.txt\": \"TB4-03059\",\n",
    "            \"2750270.txt\": \"MB4-01799\",\n",
    "            \"2776412.txt\": \"TB6-08260\",\n",
    "            \"2901493.txt\": \"TB8-05152\",\n",
    "            \"2954037.txt\": \"TB7-20791\",\n",
    "            \"2945923.txt\": \"TB7-22410\",\n",
    "            \"2953978.txt\": \"TB8-17738\",\n",
    "            \"2996850.txt\": \"TB7-02952\",\n",
    "            \"3060429.txt\": \"MB7-00268\",\n",
    "            \"3064579.txt\": \"MB8-07401\",\n",
    "            \"3066742.txt\": \"TB9-05486\",\n",
    "            \"3074204.txt\": \"TB8-19298\",\n",
    "            \"3081729.txt\": \"TB8-04755\",\n",
    "            \"3009899.txt\": \"MB7-05420\",\n",
    "            \"3091994.txt\": \"TB8-22918\",\n",
    "            \"3112189.txt\": \"TB9-01803\",\n",
    "            \"3112241.txt\": \"MB9-01099\",\n",
    "            \"3135686.txt\": \"TB8-06759\",  \n",
    "            \"3131897.txt\": \"TB9-06111\",\n",
    "            \"3169372.txt\": \"MB9-05816\",\n",
    "            \"3169471.txt\": \"TB9-06069\",\n",
    "            \"3169485.txt\": \"TB9-15836\",\n",
    "            \"3172931.txt\": \"MB6-03390\",\n",
    "            \"3173907.txt\": \"TB9-21518\",\n",
    "            \"3176472.txt\": \"TB9-19840\",\n",
    "            \"3178266.txt\": \"VB9-05507\",\n",
    "            \"3270723.txt\": \"VB9-05051\",\n",
    "            \"3386768.txt\": \"TB9-32990\",\n",
    "            \"3386786.txt\": \"VB9-06946\",\n",
    "            \"3399587.txt\": \"MB9-11935\",\n",
    "            \"3436433.txt\": \"VB9-08997\",\n",
    "            \"3444153.txt\": \"TC0-01391\",\n",
    "            \"3457769.txt\": \"TB9-01269\",\n",
    "            \"3514437.txt\": \"TB9-33272\",\n",
    "            \"3532711.txt\": \"MB9-28671\",\n",
    "            \"3546032.txt\": \"MB9-04262\",\n",
    "            \"3546108.txt\": \"MB9-30244\",\n",
    "            \"3546284.txt\": \"TB9-08176\",\n",
    "            \"3546335.txt\": \"TB9-35398\",\n",
    "            \"3567362.txt\": \"TB9-35626\",\n",
    "            \"3581415.txt\": \"MB9-01194\",\n",
    "            \"3594465.txt\": \"TB9-03384\",\n",
    "                        \n",
    "        }\n",
    "\n",
    "        # Check for problem files using a loop, then extract RAD number if not found\n",
    "        rad_number = None\n",
    "        for fragment, rad in problem_files.items():\n",
    "            if fragment in file_path:\n",
    "                rad_number = rad\n",
    "                break\n",
    "\n",
    "        if rad_number is None:\n",
    "            rad_number = extract_rad_number(content)\n",
    "\n",
    "        if rad_number:\n",
    "            lang = detect_language(content)\n",
    "            \n",
    "            # address problem file, else extract document date:\n",
    "            if \"TC1-10832tf\" in file_path:\n",
    "                document_date = \"2022-01-20\"\n",
    "            else:\n",
    "                document_date = extract_document_date(content)\n",
    "            \n",
    "            scraped_timestamp = datetime.fromtimestamp(os.path.getmtime(file_path), tz=timezone.utc)\n",
    "\n",
    "            return {\n",
    "                'citation': rad_number,\n",
    "                'citation2': '',\n",
    "                'dataset': 'RAD',\n",
    "                'name': '',\n",
    "                'language': lang,\n",
    "                'document_date': document_date,\n",
    "                'url': os.path.basename(file_path),\n",
    "                'scraped_timestamp':  scraped_timestamp,\n",
    "                'unofficial_text': content,\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        #raise Exception(f\"Error processing file {file_path}: {e}\") ### uncomment to see error if running in dask\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Files\n",
    "This block of code reads each file in the dataset directories to extract the needed information, using the previously defined functions and form a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 246.05 s\n"
     ]
    }
   ],
   "source": [
    "# Main data processing loop (run in paraellel using Dask)\n",
    "\n",
    "def process_file_wrapper(file_path):\n",
    "    \n",
    "    if not os.path.basename(file_path).startswith('~'):\n",
    "        return process_file(file_path)\n",
    "\n",
    "# Gather all file paths\n",
    "file_paths = []\n",
    "for data_dir in DATA_DIRS:\n",
    "    if os.path.exists(data_dir) and os.path.isdir(data_dir):\n",
    "        dir_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "        file_paths.extend(dir_files)\n",
    "\n",
    "# Create a Dask Bag from file paths\n",
    "file_bag = db.from_sequence(file_paths)\n",
    "\n",
    "# Use Dask to process files in parallel\n",
    "with ProgressBar():\n",
    "    results = file_bag.map(process_file_wrapper).filter(lambda x: x is not None).compute()\n",
    "\n",
    "# Convert results to a Pandas DataFrame\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_backup.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "Cleans data to match huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting df 29440\n",
      "Number of rows after manually removing some files: 29433\n",
      "Number of rows with no document_date: 0\n",
      "Before removing before 2013 29433\n",
      "[]\n",
      "After removing before 2013 29433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>citation2</th>\n",
       "      <th>dataset</th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "      <th>document_date</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_timestamp</th>\n",
       "      <th>unofficial_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MC3-00542</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2024-06-14 00:00:00+00:00</td>\n",
       "      <td>2. MC3-00542tf.txt</td>\n",
       "      <td>2024-09-10 01:13:43.447176+00:00</td>\n",
       "      <td>\\nDossier de la SAR / RAD File: MC3-00542\\n\\nH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MC3-00542</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2024-06-14 00:00:00+00:00</td>\n",
       "      <td>1. MC3-00542 (Ouellet - Mexico - 111(1)(a) - D...</td>\n",
       "      <td>2024-09-10 01:13:43.141365+00:00</td>\n",
       "      <td>\\nRAD File / Dossier de la SAR : MC3-00542\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VC3-14129</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2024-03-04 00:00:00+00:00</td>\n",
       "      <td>VC3-14129tf.txt</td>\n",
       "      <td>2024-07-12 18:44:44.255575+00:00</td>\n",
       "      <td>\\nDossier de la SAR / RAD File: VC3-14129\\n\\nH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VC3-14129</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2024-03-04 00:00:00+00:00</td>\n",
       "      <td>VC3-14129 a.txt</td>\n",
       "      <td>2024-07-12 18:44:43.974644+00:00</td>\n",
       "      <td>\\nRAD File / Dossier de la SAR : VC3-14129\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VC2-05699</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2024-02-21 00:00:00+00:00</td>\n",
       "      <td>VC2-05699tf.txt</td>\n",
       "      <td>2024-07-12 18:44:42.078996+00:00</td>\n",
       "      <td>\\nDossier de la SAR / RAD File: VC2-05699\\n\\nH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29428</th>\n",
       "      <td>VB3-00460</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-02-27 00:00:00+00:00</td>\n",
       "      <td>1519098.txt</td>\n",
       "      <td>2023-11-13 01:12:53.391303+00:00</td>\n",
       "      <td>\\n\\n\\n\\nN° de dossier de la SAR / RAD File No....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29429</th>\n",
       "      <td>VB3-00316</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-02-26 00:00:00+00:00</td>\n",
       "      <td>1526727.txt</td>\n",
       "      <td>2023-11-13 01:46:02.264031+00:00</td>\n",
       "      <td>\\n\\n\\n\\nN° de dossier de la SAR / RAD File No....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29430</th>\n",
       "      <td>VB3-00316</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2013-02-26 00:00:00+00:00</td>\n",
       "      <td>1526685.txt</td>\n",
       "      <td>2023-11-13 01:16:17.679784+00:00</td>\n",
       "      <td>\\n\\n\\n\\nRAD File No. / N° de dossier de la SAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29431</th>\n",
       "      <td>VB3-00389</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2013-02-19 00:00:00+00:00</td>\n",
       "      <td>1526712.txt</td>\n",
       "      <td>2023-11-13 01:46:01.151671+00:00</td>\n",
       "      <td>\\n\\n\\n\\nRAD File No. / N° de dossier de la SAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29432</th>\n",
       "      <td>VB3-00389</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-02-19 00:00:00+00:00</td>\n",
       "      <td>1526698.txt</td>\n",
       "      <td>2023-11-13 01:46:00.757913+00:00</td>\n",
       "      <td>\\n\\n\\n\\nN° de dossier de la SAR / RAD File No....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29433 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citation citation2 dataset name language             document_date   \n",
       "0      MC3-00542               RAD            fr 2024-06-14 00:00:00+00:00  \\\n",
       "1      MC3-00542               RAD            en 2024-06-14 00:00:00+00:00   \n",
       "2      VC3-14129               RAD            fr 2024-03-04 00:00:00+00:00   \n",
       "3      VC3-14129               RAD            en 2024-03-04 00:00:00+00:00   \n",
       "4      VC2-05699               RAD            fr 2024-02-21 00:00:00+00:00   \n",
       "...          ...       ...     ...  ...      ...                       ...   \n",
       "29428  VB3-00460               RAD            fr 2013-02-27 00:00:00+00:00   \n",
       "29429  VB3-00316               RAD            fr 2013-02-26 00:00:00+00:00   \n",
       "29430  VB3-00316               RAD            en 2013-02-26 00:00:00+00:00   \n",
       "29431  VB3-00389               RAD            en 2013-02-19 00:00:00+00:00   \n",
       "29432  VB3-00389               RAD            fr 2013-02-19 00:00:00+00:00   \n",
       "\n",
       "                                                     url   \n",
       "0                                     2. MC3-00542tf.txt  \\\n",
       "1      1. MC3-00542 (Ouellet - Mexico - 111(1)(a) - D...   \n",
       "2                                        VC3-14129tf.txt   \n",
       "3                                        VC3-14129 a.txt   \n",
       "4                                        VC2-05699tf.txt   \n",
       "...                                                  ...   \n",
       "29428                                        1519098.txt   \n",
       "29429                                        1526727.txt   \n",
       "29430                                        1526685.txt   \n",
       "29431                                        1526712.txt   \n",
       "29432                                        1526698.txt   \n",
       "\n",
       "                     scraped_timestamp   \n",
       "0     2024-09-10 01:13:43.447176+00:00  \\\n",
       "1     2024-09-10 01:13:43.141365+00:00   \n",
       "2     2024-07-12 18:44:44.255575+00:00   \n",
       "3     2024-07-12 18:44:43.974644+00:00   \n",
       "4     2024-07-12 18:44:42.078996+00:00   \n",
       "...                                ...   \n",
       "29428 2023-11-13 01:12:53.391303+00:00   \n",
       "29429 2023-11-13 01:46:02.264031+00:00   \n",
       "29430 2023-11-13 01:16:17.679784+00:00   \n",
       "29431 2023-11-13 01:46:01.151671+00:00   \n",
       "29432 2023-11-13 01:46:00.757913+00:00   \n",
       "\n",
       "                                         unofficial_text  \n",
       "0      \\nDossier de la SAR / RAD File: MC3-00542\\n\\nH...  \n",
       "1      \\nRAD File / Dossier de la SAR : MC3-00542\\n\\n...  \n",
       "2      \\nDossier de la SAR / RAD File: VC3-14129\\n\\nH...  \n",
       "3      \\nRAD File / Dossier de la SAR : VC3-14129\\n\\n...  \n",
       "4      \\nDossier de la SAR / RAD File: VC2-05699\\n\\nH...  \n",
       "...                                                  ...  \n",
       "29428  \\n\\n\\n\\nN° de dossier de la SAR / RAD File No....  \n",
       "29429  \\n\\n\\n\\nN° de dossier de la SAR / RAD File No....  \n",
       "29430  \\n\\n\\n\\nRAD File No. / N° de dossier de la SAR...  \n",
       "29431  \\n\\n\\n\\nRAD File No. / N° de dossier de la SAR...  \n",
       "29432  \\n\\n\\n\\nN° de dossier de la SAR / RAD File No....  \n",
       "\n",
       "[29433 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Starting df\", len(df))\n",
    "\n",
    "# manually fix some document_date issues\n",
    "df.loc[df['url'] == 'TB8-11442f.txt', 'document_date'] = pd.Timestamp('2020-10-15', tz='UTC')\n",
    "df.loc[df['url'] == '1820460.txt', 'document_date'] = pd.Timestamp('2014-04-24', tz='UTC')\n",
    "df.loc[df['url'] == '2876788.txt', 'document_date'] = pd.Timestamp('2018-04-05', tz='UTC')\n",
    "df.loc[df['url'] == '3196706.txt', 'document_date'] = pd.Timestamp('2019-11-13', tz='UTC')\n",
    "df.loc[df['url'] == '3196826.txt', 'document_date'] = pd.Timestamp('2019-10-28', tz='UTC')\n",
    "df.loc[df['url'] == '3546110.txt', 'document_date'] = pd.Timestamp('2021-01-21', tz='UTC')\n",
    "df.loc[df['url'] == '3389310.txt', 'document_date'] = pd.Timestamp('2021-04-20', tz='UTC')\n",
    "df.loc[df['url'] == 'MC0-05967f.txt', 'document_date'] = pd.Timestamp('2021-04-20', tz='UTC')\n",
    "df.loc[df['url'] == '3061334.txt', 'document_date'] = pd.Timestamp('2019-02-05', tz='UTC')\n",
    "df.loc[df['url'] == 'MC1-12211 a.txt', 'document_date'] = pd.Timestamp('2022-08-19', tz='UTC')\n",
    "df.loc[df['url'] == 'TC0-10646tf.txt', 'document_date'] = pd.Timestamp('2021-11-17', tz='UTC')\n",
    "df.loc[df['url'] == 'TC1-10832 a.txt', 'document_date'] = pd.Timestamp('2022-01-20', tz='UTC')\n",
    "df.loc[df['url'] == 'TC1-14698tf.txt', 'document_date'] = pd.Timestamp('2022-01-19', tz='UTC')\n",
    "df.loc[df['url'] == 'TC1-14698 a.txt', 'document_date'] = pd.Timestamp('2022-01-19', tz='UTC')\n",
    "df.loc[df['url'] == 'MB9-26873tf.txt', 'document_date'] = pd.Timestamp('2022-03-18', tz='UTC')\n",
    "df.loc[df['url'] == 'MB9-26873a.txt', 'document_date'] = pd.Timestamp('2022-03-18', tz='UTC')\n",
    "df.loc[df['url'] == 'MB9-22375tf.txt', 'document_date'] = pd.Timestamp('2020-09-03', tz='UTC')\n",
    "df.loc[df['url'] == 'MB9-22375a.txt', 'document_date'] = pd.Timestamp('2020-09-03', tz='UTC')\n",
    "df.loc[df['url'] == '2769752.txt', 'document_date'] = pd.Timestamp('2016-07-18', tz='UTC')\n",
    "df.loc[df['url'] == '2797762.txt', 'document_date'] = pd.Timestamp('2016-07-04', tz='UTC')\n",
    "df.loc[df['url'] == '2892390.txt', 'document_date'] = pd.Timestamp('2018-07-11', tz='UTC')\n",
    "df.loc[df['url'] == '3567370.txt', 'document_date'] = pd.Timestamp('2021-02-01', tz='UTC')\n",
    "df.loc[df['url'] == '3566569.txt', 'document_date'] = pd.Timestamp('2021-01-30', tz='UTC')\n",
    "df.loc[df['url'] == '3479804.txt', 'document_date'] = pd.Timestamp('2021-01-25', tz='UTC')\n",
    "df.loc[df['url'] == '3479803.txt', 'document_date'] = pd.Timestamp('2021-01-25', tz='UTC')\n",
    "df.loc[df['url'] == '3479768.txt', 'document_date'] = pd.Timestamp('2021-01-11', tz='UTC')\n",
    "df.loc[df['url'] == '3479767.txt', 'document_date'] = pd.Timestamp('2021-01-11', tz='UTC')\n",
    "df.loc[df['url'] == '3477399.txt', 'document_date'] = pd.Timestamp('2021-01-13', tz='UTC')\n",
    "df.loc[df['url'] == '3477398.txt', 'document_date'] = pd.Timestamp('2021-01-13', tz='UTC')\n",
    "df.loc[df['url'] == '3444104.txt', 'document_date'] = pd.Timestamp('2021-02-02', tz='UTC')\n",
    "df.loc[df['url'] == '3196782.txt', 'document_date'] = pd.Timestamp('2019-11-01', tz='UTC')\n",
    "df.loc[df['url'] == '3170609.txt', 'document_date'] = pd.Timestamp('2019-05-30', tz='UTC')\n",
    "df.loc[df['url'] == '3170608.txt', 'document_date'] = pd.Timestamp('2019-05-30', tz='UTC')\n",
    "df.loc[df['url'] == '3170577.txt', 'document_date'] = pd.Timestamp('2019-05-10', tz='UTC')\n",
    "df.loc[df['url'] == '3112268.txt', 'document_date'] = pd.Timestamp('2019-06-24', tz='UTC')\n",
    "df.loc[df['url'] == '3112267.txt', 'document_date'] = pd.Timestamp('2019-06-24', tz='UTC')\n",
    "df.loc[df['url'] == '3091928.txt', 'document_date'] = pd.Timestamp('2019-07-09', tz='UTC')\n",
    "df.loc[df['url'] == '3061306.txt', 'document_date'] = pd.Timestamp('2018-09-27', tz='UTC')\n",
    "df.loc[df['url'] == '3036997.txt', 'document_date'] = pd.Timestamp('2019-01-28', tz='UTC')\n",
    "df.loc[df['url'] == '3036996.txt', 'document_date'] = pd.Timestamp('2019-01-28', tz='UTC')\n",
    "df.loc[df['url'] == '3009983.txt', 'document_date'] = pd.Timestamp('2018-09-18', tz='UTC')\n",
    "df.loc[df['url'] == '3009982.txt', 'document_date'] = pd.Timestamp('2018-09-18', tz='UTC')\n",
    "df.loc[df['url'] == '2883691.txt', 'document_date'] = pd.Timestamp('2018-01-31', tz='UTC')\n",
    "df.loc[df['url'] == '2868255.txt', 'document_date'] = pd.Timestamp('2017-12-28', tz='UTC')\n",
    "df.loc[df['url'] == '2868191.txt', 'document_date'] = pd.Timestamp('2018-01-15', tz='UTC')\n",
    "df.loc[df['url'] == '2868190.txt', 'document_date'] = pd.Timestamp('2018-01-15', tz='UTC')\n",
    "df.loc[df['url'] == '2834981.txt', 'document_date'] = pd.Timestamp('2018-02-19', tz='UTC')\n",
    "df.loc[df['url'] == '2834981.txt', 'document_date'] = pd.Timestamp('2018-02-19', tz='UTC')\n",
    "df.loc[df['url'] == '1844495.txt', 'document_date'] = pd.Timestamp('2014-02-17', tz='UTC')\n",
    "df.loc[df['url'] == '1844494.txt', 'document_date'] = pd.Timestamp('2014-02-17', tz='UTC')\n",
    "df.loc[df['url'] == '1831965.txt', 'document_date'] = pd.Timestamp('2014-11-05', tz='UTC')\n",
    "df.loc[df['url'] == '1825449.txt', 'document_date'] = pd.Timestamp('2014-12-05', tz='UTC')\n",
    "df.loc[df['url'] == '1825448.txt', 'document_date'] = pd.Timestamp('2014-12-05', tz='UTC')\n",
    "df.loc[df['url'] == '1797868.txt', 'document_date'] = pd.Timestamp('2015-04-13', tz='UTC')\n",
    "df.loc[df['url'] == '1757086.txt', 'document_date'] = pd.Timestamp('2015-03-02', tz='UTC')\n",
    "df.loc[df['url'] == '1577641.txt', 'document_date'] = pd.Timestamp('2014-06-23', tz='UTC')\n",
    "df.loc[df['url'] == '1563624.txt', 'document_date'] = pd.Timestamp('2014-04-07', tz='UTC')\n",
    "df.loc[df['url'] == '1563601.txt', 'document_date'] = pd.Timestamp('2014-04-07', tz='UTC')\n",
    "df.loc[df['url'] == '1526721.txt', 'document_date'] = pd.Timestamp('2013-10-18', tz='UTC')\n",
    "df.loc[df['url'] == '3091929.txt', 'document_date'] = pd.Timestamp('2019-06-11', tz='UTC')\n",
    "df.loc[df['url'] == '2757789.txt', 'document_date'] = pd.Timestamp('2016-02-23', tz='UTC')\n",
    "df.loc[df['url'] == '2901493.txt', 'document_date'] = pd.Timestamp('2018-03-16', tz='UTC')\n",
    "df.loc[df['url'] == '3546053.txt', 'document_date'] = pd.Timestamp('2021-01-12', tz='UTC')\n",
    "df.loc[df['url'] == '3229927.txt', 'document_date'] = pd.Timestamp('2020-01-24', tz='UTC')\n",
    "df.loc[df['url'] == '3229926.txt', 'document_date'] = pd.Timestamp('2020-01-24', tz='UTC')\n",
    "\n",
    "# remove a few rows where there is no matching other language decision, but multiple versions\n",
    "df = df[~((df['url'] == '2928118.txt'))]  # fr one is missing, but en one is there + multiple\n",
    "df = df[~((df['url'] == '3074046.txt'))]  # fr one is missing, but en one is there + multiple\n",
    "df = df[~((df['url'] == '2746200.txt'))]  # fr one is missing, but en one is there + multiple\n",
    "df = df[~((df['url'] == '3074119.txt'))]  # fr one is missing, but en one is there + multiple\n",
    "df = df[~((df['url'] == '3078334.txt'))]  # fr one is missing, but en one is there + multipl\n",
    "df = df[~((df['url'] == '2897171.txt'))]  # two fr same, no en, multiple. \n",
    "df = df[~((df['url'] == '2897172.txt'))]  # two fr same, no en, multiple.\n",
    "print(\"Number of rows after manually removing some files:\", len(df))\n",
    "\n",
    "# manually fix some language issues\n",
    "df.loc[df['url'] == '1849447.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1529926.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1831945.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1831957.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1698426.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1526185.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1691141.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1691145.txt', 'language'] = 'fr'\n",
    "df.loc[df['url'] == '1849475.txt', 'language'] = 'fr'\n",
    "\n",
    "# fix dates format\n",
    "df['document_date'] = pd.to_datetime(df['document_date'], errors='coerce', utc=True)\n",
    "\n",
    "# Sort by document_date descending, so newest dates are first; NaT is last\n",
    "df = df.sort_values(by=['document_date', 'scraped_timestamp'], ascending=[False, False])\n",
    "\n",
    "# print number of rows with no document_date\n",
    "print(\"Number of rows with no document_date:\", df['document_date'].isna().sum())\n",
    "\n",
    "\n",
    "# export to parquet the rows where there is no document_date\n",
    "df_no_date = df[df['document_date'].isna()]\n",
    "df_no_date.to_parquet(\"DATA/df_no_date.parquet\", index=False)\n",
    "\n",
    "# Remove rows where document_date is before 2013\n",
    "print(\"Before removing before 2013\", len(df))\n",
    "# print the rows where df[df['document_date'] >= pd.Timestamp('2013-01-01', tz='UTC')].reset_index(drop=True)\n",
    "print(df[df['document_date'] < pd.Timestamp('2013-01-01', tz='UTC')]['url'].tolist())\n",
    "\n",
    "df = df[df['document_date'] >= pd.Timestamp('2013-01-01', tz='UTC')].reset_index(drop=True)\n",
    "\n",
    "print (\"After removing before 2013\", len(df)) \n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29433/29433 [00:40<00:00, 731.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clean text of cases\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove \\xa0\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # Remove multiple whitespaces and preserve paragraphs\n",
    "    text = '\\n'.join([re.sub(r'\\s+', ' ', line.strip()) for line in text.split('\\n')])\n",
    "    \n",
    "    # # Remove single newlines\n",
    "    # text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "    # Convert multiple newlines to single newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Remove all strings '\\n[Page #]\\n' (with # being a number of up to 4 digits \n",
    "    text = re.sub(r'\\n\\[Page \\d{1,3}\\]\\n', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "df['unofficial_text'] = df.unofficial_text.progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated citations in df_en: 808\n",
      "Number of duplicated citations in df_fr: 808\n",
      "Number of duplicated rows in df_en for BOTH citation and document_date: 233\n",
      "Number of duplicated rows in df_fr for BOTH citation and document_date: 235\n",
      "Number of rows in df_en after removing all duplicate citations: 13906\n",
      "Number of rows in df_fr after removing duplicate citations: 13910\n",
      "Num rows in df_en with no citation match in df_fr 28\n",
      "Num rows in df_fr with no citation match in df_en 32\n",
      "Number of rows in final df: 13878\n",
      "*********** AT SOME POINT RECONSIDER THIS ***********\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_en</th>\n",
       "      <th>citation2_en</th>\n",
       "      <th>dataset</th>\n",
       "      <th>name_en</th>\n",
       "      <th>document_date_en</th>\n",
       "      <th>url_en</th>\n",
       "      <th>scraped_timestamp_en</th>\n",
       "      <th>unofficial_text_en</th>\n",
       "      <th>citation_fr</th>\n",
       "      <th>citation2_fr</th>\n",
       "      <th>name_fr</th>\n",
       "      <th>document_date_fr</th>\n",
       "      <th>url_fr</th>\n",
       "      <th>scraped_timestamp_fr</th>\n",
       "      <th>unofficial_text_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MB3-00631</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>2013-03-04 00:00:00+00:00</td>\n",
       "      <td>1506095.txt</td>\n",
       "      <td>2023-11-13 01:11:22.214474+00:00</td>\n",
       "      <td>\\nImmigration and\\nRefugee Board of Canada\\nRe...</td>\n",
       "      <td>MB3-00631</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2013-03-04 00:00:00+00:00</td>\n",
       "      <td>1506092.txt</td>\n",
       "      <td>2023-11-13 01:11:22.014576+00:00</td>\n",
       "      <td>\\nCommission de l'immigration et du statut de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MB3-00772</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>2013-03-13 00:00:00+00:00</td>\n",
       "      <td>1506098.txt</td>\n",
       "      <td>2023-11-13 01:11:22.347155+00:00</td>\n",
       "      <td>\\nImmigration and\\nRefugee Board of Canada\\nRe...</td>\n",
       "      <td>MB3-00772</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2013-03-13 00:00:00+00:00</td>\n",
       "      <td>1506097.txt</td>\n",
       "      <td>2023-11-13 01:11:22.281050+00:00</td>\n",
       "      <td>\\nCommission de l'immigration et du statut de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MB3-00899</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>2013-03-21 00:00:00+00:00</td>\n",
       "      <td>1415668.txt</td>\n",
       "      <td>2023-11-13 01:10:09.061743+00:00</td>\n",
       "      <td>\\nImmigration and\\nRefugee Board of Canada\\nRe...</td>\n",
       "      <td>MB3-00899</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2013-03-21 00:00:00+00:00</td>\n",
       "      <td>1415666.txt</td>\n",
       "      <td>2023-11-13 01:10:08.900619+00:00</td>\n",
       "      <td>\\nCommission de l'immigration et du statut de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MB3-00900</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>2013-03-21 00:00:00+00:00</td>\n",
       "      <td>1507941.txt</td>\n",
       "      <td>2023-11-13 01:11:58.181480+00:00</td>\n",
       "      <td>\\nRAD File No. / N° de dossier de la SAR : MB3...</td>\n",
       "      <td>MB3-00900</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2013-03-21 00:00:00+00:00</td>\n",
       "      <td>1507922.txt</td>\n",
       "      <td>2023-11-13 01:11:57.481938+00:00</td>\n",
       "      <td>\\nCommission de l'immigration et du statut de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MB3-01185</td>\n",
       "      <td></td>\n",
       "      <td>RAD</td>\n",
       "      <td></td>\n",
       "      <td>2013-07-09 00:00:00+00:00</td>\n",
       "      <td>1414390.txt</td>\n",
       "      <td>2023-11-13 01:10:06.703031+00:00</td>\n",
       "      <td>\\nRAD File No. / N° de dossier de la SAR : MB3...</td>\n",
       "      <td>MB3-01185</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2013-07-09 00:00:00+00:00</td>\n",
       "      <td>1414397.txt</td>\n",
       "      <td>2023-11-13 01:10:06.914564+00:00</td>\n",
       "      <td>\\nN° de dossier de la SAR/RAD File No.: MB3-01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  citation_en citation2_en dataset name_en          document_date_en   \n",
       "0   MB3-00631                  RAD         2013-03-04 00:00:00+00:00  \\\n",
       "1   MB3-00772                  RAD         2013-03-13 00:00:00+00:00   \n",
       "2   MB3-00899                  RAD         2013-03-21 00:00:00+00:00   \n",
       "3   MB3-00900                  RAD         2013-03-21 00:00:00+00:00   \n",
       "4   MB3-01185                  RAD         2013-07-09 00:00:00+00:00   \n",
       "\n",
       "        url_en             scraped_timestamp_en   \n",
       "0  1506095.txt 2023-11-13 01:11:22.214474+00:00  \\\n",
       "1  1506098.txt 2023-11-13 01:11:22.347155+00:00   \n",
       "2  1415668.txt 2023-11-13 01:10:09.061743+00:00   \n",
       "3  1507941.txt 2023-11-13 01:11:58.181480+00:00   \n",
       "4  1414390.txt 2023-11-13 01:10:06.703031+00:00   \n",
       "\n",
       "                                  unofficial_text_en citation_fr citation2_fr   \n",
       "0  \\nImmigration and\\nRefugee Board of Canada\\nRe...   MB3-00631               \\\n",
       "1  \\nImmigration and\\nRefugee Board of Canada\\nRe...   MB3-00772                \n",
       "2  \\nImmigration and\\nRefugee Board of Canada\\nRe...   MB3-00899                \n",
       "3  \\nRAD File No. / N° de dossier de la SAR : MB3...   MB3-00900                \n",
       "4  \\nRAD File No. / N° de dossier de la SAR : MB3...   MB3-01185                \n",
       "\n",
       "  name_fr          document_date_fr       url_fr   \n",
       "0         2013-03-04 00:00:00+00:00  1506092.txt  \\\n",
       "1         2013-03-13 00:00:00+00:00  1506097.txt   \n",
       "2         2013-03-21 00:00:00+00:00  1415666.txt   \n",
       "3         2013-03-21 00:00:00+00:00  1507922.txt   \n",
       "4         2013-07-09 00:00:00+00:00  1414397.txt   \n",
       "\n",
       "              scraped_timestamp_fr   \n",
       "0 2023-11-13 01:11:22.014576+00:00  \\\n",
       "1 2023-11-13 01:11:22.281050+00:00   \n",
       "2 2023-11-13 01:10:08.900619+00:00   \n",
       "3 2023-11-13 01:11:57.481938+00:00   \n",
       "4 2023-11-13 01:10:06.914564+00:00   \n",
       "\n",
       "                                  unofficial_text_fr  \n",
       "0  \\nCommission de l'immigration et du statut de ...  \n",
       "1  \\nCommission de l'immigration et du statut de ...  \n",
       "2  \\nCommission de l'immigration et du statut de ...  \n",
       "3  \\nCommission de l'immigration et du statut de ...  \n",
       "4  \\nN° de dossier de la SAR/RAD File No.: MB3-01...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get df_en where language is English\n",
    "df_en = df[df['language'] == 'en'].reset_index(drop=True)\n",
    "# get df_fr where language is French\n",
    "df_fr = df[df['language'] == 'fr'].reset_index(drop=True)\n",
    "\n",
    "# print how many df_en.citation is duplicated\n",
    "print(\"Number of duplicated citations in df_en:\", df_en.citation.duplicated().sum())\n",
    "print(\"Number of duplicated citations in df_fr:\", df_fr.citation.duplicated().sum())\n",
    "\n",
    "# print how many rows in df_en are duplicated for BOTH citation and document_date\n",
    "print(\"Number of duplicated rows in df_en for BOTH citation and document_date:\", df_en.duplicated(subset=['citation', 'document_date']).sum())\n",
    "print(\"Number of duplicated rows in df_fr for BOTH citation and document_date:\", df_fr.duplicated(subset=['citation', 'document_date']).sum())\n",
    "\n",
    "# export to parquet the rows where there are duplicated citations in df_en\n",
    "df_en_duplicates = df_en[df_en.duplicated(subset=['citation'], keep=False)]\n",
    "df_en_duplicates.to_parquet(\"DATA/df_en_duplicates.parquet\", index=False)\n",
    "# export to parquet the rows where there are duplicated citations in df_fr\n",
    "df_fr_duplicates = df_fr[df_fr.duplicated(subset=['citation'], keep=False)]\n",
    "df_fr_duplicates.to_parquet(\"DATA/df_fr_duplicates.parquet\", index=False)   \n",
    "\n",
    "# Blunt removal of duplicates in df_en and df_fr\n",
    "df_en = df_en.sort_values(by=['citation', 'document_date', 'scraped_timestamp'], ascending=[True, False, False])\n",
    "df_en = df_en.drop_duplicates(subset=['unofficial_text'], keep='first').reset_index(drop=True)\n",
    "df_en = df_en.sort_values(by=['citation', 'document_date', 'scraped_timestamp'], ascending=[True, False, False])\n",
    "df_en = df_en.drop_duplicates(subset=['citation'], keep='first').reset_index(drop=True)\n",
    "\n",
    "df_fr = df_fr.sort_values(by=['citation', 'document_date', 'scraped_timestamp'], ascending=[True, False, False])\n",
    "df_fr = df_fr.drop_duplicates(subset=['unofficial_text'], keep='first').reset_index(drop=True)\n",
    "df_fr = df_fr.sort_values(by=['citation', 'document_date', 'scraped_timestamp'], ascending=[True, False, False])\n",
    "df_fr = df_fr.drop_duplicates(subset=['citation'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Print the number of rows after removing duplicates\n",
    "print(\"Number of rows in df_en after removing all duplicate citations:\", len(df_en))\n",
    "print(\"Number of rows in df_fr after removing duplicate citations:\", len(df_fr))\n",
    "\n",
    "\n",
    "# number of rows in df_en that does not have an identical citation in df_fr\n",
    "print(\n",
    "    \"Num rows in df_en with no citation match in df_fr\",\n",
    "    df_en[~df_en['citation'].isin(df_fr['citation'])].reset_index(drop=True).shape[0]\n",
    ")\n",
    "# number of rows in df_fr that does not have an identical citation in df_en\n",
    "print(\n",
    "    \"Num rows in df_fr with no citation match in df_en\",\n",
    "    df_fr[~df_fr['citation'].isin(df_en['citation'])].reset_index(drop=True).shape[0]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# for df_en add \"_en\" to end of all cols\n",
    "df_en.columns = [col + \"_en\" for col in df_en.columns]\n",
    "# for df_fr add \"_fr\" to end of all cols\n",
    "df_fr.columns = [col + \"_fr\" for col in df_fr.columns]\n",
    "\n",
    "# merge df_en and df_fr on citation_en and citation_fr, keeping only matches\n",
    "df = pd.merge(df_en, df_fr, left_on='citation_en', right_on='citation_fr', how='inner')\n",
    "\n",
    "# rename dataset_en to dataset, and drop unused cols\n",
    "df = df.rename(columns={'dataset_en': 'dataset'})\n",
    "df = df.drop(columns=['dataset_fr', 'language_en', 'language_fr'])\n",
    "\n",
    "\n",
    "# print len of final df\n",
    "print(\"Number of rows in final df:\", len(df))\n",
    "print(\"*********** AT SOME POINT RECONSIDER THIS ***********\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df with different document_date_en and document_date_fr: 0\n"
     ]
    }
   ],
   "source": [
    "# how many rows in df have document_date_en and document_date_fr that are not the same\n",
    "print(\"Number of rows in df with different document_date_en and document_date_fr:\", \n",
    "      df[df['document_date_en'] != df['document_date_fr']].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_en</th>\n",
       "      <th>citation2_en</th>\n",
       "      <th>dataset</th>\n",
       "      <th>name_en</th>\n",
       "      <th>document_date_en</th>\n",
       "      <th>url_en</th>\n",
       "      <th>scraped_timestamp_en</th>\n",
       "      <th>unofficial_text_en</th>\n",
       "      <th>citation_fr</th>\n",
       "      <th>citation2_fr</th>\n",
       "      <th>name_fr</th>\n",
       "      <th>document_date_fr</th>\n",
       "      <th>url_fr</th>\n",
       "      <th>scraped_timestamp_fr</th>\n",
       "      <th>unofficial_text_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [citation_en, citation2_en, dataset, name_en, document_date_en, url_en, scraped_timestamp_en, unofficial_text_en, citation_fr, citation2_fr, name_fr, document_date_fr, url_fr, scraped_timestamp_fr, unofficial_text_fr]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['document_date_en'] != df['document_date_fr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: CHECKED THIS ONE, MISSING: \n",
    "# MB3-02526\t \n",
    "# MB7-05420 \n",
    "# MB7-07448\n",
    "# MB7-09115\n",
    "# MB7-15795\t### THERE'S A BUNCH FROM 2019 THAT ARE MISSING FROM MY FILES\n",
    "# MB7-20800\t\n",
    "# MB7-21542\t\n",
    "# MB7-22590\t\n",
    "# MB7-24208\t\n",
    "# MB8-01099\n",
    "# MB8-03922\t\n",
    "# TB4-00097\n",
    "# TB4-00557\n",
    "# TB4-03241\t\n",
    "# TB6-11497\t\n",
    "# TB8‑02686\n",
    "# TB8-05513\t\n",
    "# TB8-08621\n",
    "# TB8-13027\n",
    "# TB8-18196\n",
    "# TB8-19024\n",
    "# TB8-32262\n",
    "# TB9‑09752\n",
    "# TB9-16802\n",
    "# VB5-02697\n",
    "# VB8-04489\t\n",
    "# VB9-02244\t\n",
    "\n",
    "# show rows where df_en.url is not in df.url_en\n",
    "df_en[~df_en['url_en'].isin(df['url_en'])].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show citation_fr where df_fr.url_fr is not in df.url_fr\n",
    "df_fr[df_fr['url_fr'].isin(df['url_fr']) == False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows where df_fr.url is not in df.url_fr\n",
    "fr_list=df_fr[~df_fr['url_fr'].isin(df['url_fr'])].reset_index(drop=True)['citation_fr'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "fr_list=df_fr[~df_fr['url_fr'].isin(df['url_fr'])].reset_index(drop=True)['citation_fr'].to_list()\n",
    "en_list=df_en[~df_en['url_en'].isin(df['url_en'])].reset_index(drop=True)['citation_en'].to_list()\n",
    "matches = []\n",
    "cutoff = 0.8\n",
    "for en_code in en_list:\n",
    "    close = difflib.get_close_matches(en_code, fr_list, n=1, cutoff=cutoff)\n",
    "    if close:\n",
    "        ratio = difflib.SequenceMatcher(None, en_code, close[0]).ratio()\n",
    "        matches.append((en_code, close[0], round(ratio, 2)))\n",
    "df_matches = pd.DataFrame(matches, columns=['citation_en', 'citation_fr_candidate', 'similarity'])\n",
    "df_matches = df_matches.sort_values('similarity', ascending=False)\n",
    "\n",
    "# Print or export\n",
    "print(df_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated citations_en in df: 0\n",
      "Number of duplicated citations_fr in df: 0\n",
      "Number of blank or null or nan citations_en in df: 0\n",
      "Number of blank or null or nan citations_fr in df: 0\n"
     ]
    }
   ],
   "source": [
    "# are there any duplicate citations_en in df?\n",
    "print(\"Number of duplicated citations_en in df:\", df.citation_en.duplicated().sum())\n",
    "# are there any duplicate citations_fr in df?\n",
    "print(\"Number of duplicated citations_fr in df:\", df.citation_fr.duplicated().sum())\n",
    "\n",
    "# are there any blank or null or nan citations_en in df?\n",
    "print(\"Number of blank or null or nan citations_en in df:\", df['citation_en'].isnull().sum())\n",
    "# are there any blank or null or nan citations_fr in df?\n",
    "print(\"Number of blank or null or nan citations_fr in df:\", df['citation_fr'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# export cleaned df to parquet\n",
    "out_path_parsed = pathlib.Path('DATA/rpd_cases.parquet')\n",
    "df.to_parquet(out_path_parsed, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to Mongo # NEED TAILSCALE ON\n",
    "from pymongo import MongoClient\n",
    "from datetime import timezone\n",
    "import numpy as np\n",
    "\n",
    "MONGO_URL = 'mongodb://rll-tr7970x:27017/'\n",
    "MONGO_DB = 'case-scraping'\n",
    "MONGO_COLLECTION = 'rad-scrapes'\n",
    "client = MongoClient(MONGO_URL, tz_aware=True, tzinfo=timezone.utc)\n",
    "db = client[MONGO_DB]\n",
    "collection = db[MONGO_COLLECTION]\n",
    "\n",
    "# drop existing collection if it exists\n",
    "if collection.count_documents({}) > 0:\n",
    "    collection.drop()\n",
    "\n",
    "def strip_nulls(recs):\n",
    "    return [{k: v for k, v in rec.items() if v is not None} for rec in recs]\n",
    "\n",
    "def sanitize_records(recs):\n",
    "    \"\"\"Return a NEW list of dicts that PyMongo can store.\"\"\"\n",
    "    return [\n",
    "        {k: _clean_scalar(v) for k, v in rec.items()}\n",
    "        for rec in recs\n",
    "    ]\n",
    "\n",
    "def _clean_scalar(v):\n",
    "    # 1) All “missing” markers: <NA>, NaN, NaT  →  None\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    \n",
    "    # 2) pandas Timestamp  →  native datetime with UTC tzinfo\n",
    "    if isinstance(v, pd.Timestamp):\n",
    "        return v.to_pydatetime().astimezone(timezone.utc)\n",
    "    \n",
    "    # 3) NumPy scalar (np.int64, np.float64, …)  →  Python int/float\n",
    "    if isinstance(v, np.generic):\n",
    "        return v.item()\n",
    "    \n",
    "    return v\n",
    "\n",
    "records = df.to_dict(orient=\"records\")\n",
    "clean_docs = strip_nulls(sanitize_records(records))\n",
    "result = collection.insert_many(clean_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To compare duplicates\n",
    "# import difflib\n",
    "\n",
    "# # Get the two texts\n",
    "# texts = df_fr[df_fr.duplicated(subset=['citation', 'document_date'], keep=False)].unofficial_text.iloc[[0,1]]\n",
    "\n",
    "# # Compute the diff\n",
    "# diff = difflib.unified_diff(\n",
    "#     texts.iloc[0].splitlines(),\n",
    "#     texts.iloc[1].splitlines(),\n",
    "#     fromfile='text0',\n",
    "#     tofile='text1',\n",
    "#     lineterm=''\n",
    "# )\n",
    "\n",
    "# # Print the diff\n",
    "# for line in diff:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Dask Bag from file paths\n",
    "# file_bag = db.from_sequence(file_paths)\n",
    "\n",
    "# def get_text(file_path):\n",
    "#     \"\"\"Read the file and return its content.\"\"\"\n",
    "#     with open(file_path, 'r', errors='replace', encoding='utf-8') as file:\n",
    "#         file_name = os.path.basename(file_path)\n",
    "#         return {\"file\": file_name, \"text\": file.read()}\n",
    "\n",
    "# # Use Dask to process files in parallel\n",
    "# with ProgressBar():\n",
    "#     results = file_bag.map(get_text).filter(lambda x: x is not None).compute()\n",
    "\n",
    "# # Convert results to a Pandas DataFrame\n",
    "# df_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if \"SAR File\" is in df_all.text\n",
    "# df_all['has_sar_file'] = df_all['text'].str.contains(\"SAR File\", na=False)\n",
    "\n",
    "# # len of df_all where has_sar_file is True\n",
    "# print(\"Number of rows in df_all with has_sar_file True:\", df_all[df_all['has_sar_file']].shape[0])\n",
    "\n",
    "# df_all[df_all['has_sar_file']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "\n",
    "Note that the try/except print doesn't necessarily work if running in jupyter due to limitations \n",
    "in DASK. So if you need to know whether there are errors, uncomment the raise error thing.\n",
    "\n",
    "\n",
    "TC1-10832tf.txt throws an error for some reason not sure why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
